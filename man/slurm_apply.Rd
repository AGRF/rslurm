% Generated by roxygen2 (4.1.1): do not edit by hand
% Please edit documentation in R/slurm_apply.R
\name{slurm_apply}
\alias{slurm_apply}
\title{Parallel execution of a function on the SLURM cluster}
\usage{
slurm_apply(f, params, nodes = 16, data_file = NULL, output = "table")
}
\arguments{
\item{f}{A function that accepts one or many single values as parameters and
returns a single value or a vector.}

\item{params}{A data frame of parameter values to apply \code{f} to. Each
column corresponds to a parameter of \code{f} (\emph{Note}: names must
match) and each row corresponds to a separate function call.}

\item{nodes}{The (maximum) number of cluster nodes to spread the calculation
over. \code{slurm_apply} automatically divides \code{params} in chunks of
approximately equal size to send to each node. Less nodes are allocated if
the parameter set is too small to use all CPUs in the requested nodes.}

\item{data_file}{The name of a R data file (created with
\code{\link[base]{save}}) that will be loaded on each node prior to
calling \code{f}. Note that objects in this file \emph{cannot} share one
of the following names: params, a_id, iend, istart, result.}

\item{output}{The output type. If \code{output = 'table'} (default), the
output of each node is coerced to a data frame and written with
\code{\link[base]{write.table}}. If \code{f} returns a R object that
cannot be coerced to a data frame, use \code{output = 'raw'}, which will
\code{\link[base]{save}} each node's output in .RData format.}
}
\value{
A \code{slurm_job} object containing the \code{file_prefix} assigned
  to temporary files created by \code{slurm_apply}, a \code{job_id} assigned
  by the SLURM cluster, the number of \code{nodes} effectively used and the
  type of \code{output} returned.
}
\description{
Use \code{slurm_apply} to calculate a function over multiple sets of
parameters in parallel, using up to 16 nodes of the SLURM cluster.
}
\details{
This function creates temporary files for the parameters data ('slr####.RData'),
the R script sent to each node ('slr####.R') and the Bash script
('slr####.sh') that launches the parallel computation. The set of input
parameters is divided in chunks sent to each node, and \code{f} is evaluated
in parallel within each node using functions from the \code{parallel} R
package.

Any other R objects (besides \code{params}) that \code{f} needs to access
should be saved in a .RData file (using \code{\link[base]{save}}) and the
name of this file should be given as the optional \code{data_file} argument.

When processing the computation job, the SLURM cluster will output two types
of files: those containing the return values of the function for each subset
of parameters ('slr####_[node_id].out') and those containing any console or
error output produced by R on each node ('slurm-[job_id]_[node_id].out').

After sending the job to the SLURM cluster, \code{slurm_apply} returns a
\code{slurm_job} object which can be used to cancel the job, get the job
status or output, and delete the temporary files associated with it. See
the description of the related functions for more details.
}
\examples{
\dontrun{
sjob <- slurm_apply(func, pars)
print_job_status(sjob) # Prints console/error output once job is completed.
func_result <- get_slurm_out(sjob) # Loads output data into R.
cleanup_files(sjob)
}
}
\seealso{
\code{\link{cancel_slurm}}, \code{\link{cleanup_files}},
  \code{\link{get_slurm_out}} and \code{\link{print_job_status}}
  which use the output of this function.
}

